{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport bs4\nimport requests\nimport lxml\n\nfrom pprint import pprint\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom urllib.parse import urljoin, urlparse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parse all websites of the Manga","metadata":{}},{"cell_type":"markdown","source":"## colored","metadata":{}},{"cell_type":"code","source":"html_text = requests.get('https://ww9.readonepiece.com/manga/one-piece-digital-colored-comics/', 'r').text\nsoup = BeautifulSoup(html_text, 'lxml')\n\ncl_chapters = soup.find_all('div', class_ = 'bg-bg-secondary p-3 rounded mb-3 shadow')\ncl_chapter_site = []\nfor chapter in cl_chapters:\n    site = chapter.find('a', class_ = 'uppercase text-xs bg-text-success text-white font-bold py-1 px-2 rounded').attrs['href']\n    cl_chapter_site.append(site)\n\ncl_chapter_site.sort()\npprint(len(cl_chapter_site))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## grayscale","metadata":{}},{"cell_type":"code","source":"html_text = requests.get('https://ww9.readonepiece.com/manga/one-piece/', 'r').text\nsoup = BeautifulSoup(html_text, 'lxml')\n\nbw_chapters = soup.find_all('div', class_ = 'bg-bg-secondary p-3 rounded mb-3 shadow')\nbw_chapter_site = []\nfor chapter in bw_chapters:\n    site = chapter.find('a', class_ = 'uppercase text-xs bg-text-success text-white font-bold py-1 px-2 rounded').attrs['href']\n    bw_chapter_site.append(site)\n\nbw_chapter_site.sort()\npprint(len(bw_chapter_site))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cl_chapters = []\nbw_chapters = []\nfor i in tqdm(range(len(cl_chapter_site))):\n    for j in range(len(bw_chapter_site)):\n        if cl_chapter_site[i][-3:None] == bw_chapter_site[j][-3:None]:\n            cl_chapters.append(cl_chapter_site[i])\n            bw_chapters.append(bw_chapter_site[j])\n        \ncl_chapters.sort()\nbw_chapters.sort()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pprint(len(cl_chapters))\npprint(len(bw_chapters))\n\npprint(cl_chapters[-3:None])\npprint(bw_chapters[-3:None])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parsing all image urls from each chapter","metadata":{}},{"cell_type":"markdown","source":"## colored","metadata":{}},{"cell_type":"code","source":"cl = []\nfor i in tqdm(range(len(cl_chapters)), \"extracting images\"):\n    cl_imgs = []\n    \n    html_text = requests.get(cl_chapters[i], 'r').text\n    soup = BeautifulSoup(html_text, 'lxml')\n\n    all_imgs = soup.find_all('div', class_ = 'text-center')\n    for img in all_imgs:\n        site = img.find(\"img\", class_ = 'mb-3 mx-auto js-page')\n        if site is not None:\n            cl_imgs.append(site['src'])\n            \n    cl = cl + cl_imgs\n            \n#     pprint(cl_imgs)\n#     pprint(cl)\n#     raise ValueError(\"done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(cl))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## grayscale","metadata":{}},{"cell_type":"code","source":"bw = []\nfor i in tqdm(range(len(bw_chapters)), \"extracting images\"):\n    bw_imgs = []\n    \n    html_text = requests.get(bw_chapters[i], 'r').text\n    soup = BeautifulSoup(html_text, 'lxml')\n\n    all_imgs = soup.find_all('div', class_ = 'text-center')\n    for img in all_imgs:\n        site = img.find(\"img\", class_ = 'mb-3 mx-auto js-page')\n        if site is not None:\n            bw_imgs.append(site['src'])    \n            \n    bw = bw + bw_imgs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(bw))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download the images","metadata":{}},{"cell_type":"code","source":"# def download(url, pathname):\n#     url = url.replace('\\r', '')\n    \n#     # if path doesn't exist, make that path dir\n#     if not os.path.isdir(pathname):\n#         os.makedirs(pathname)\n\n#     filename = os.path.join(pathname, url.split(\"/\")[-1])\n#     with open(filename, \"wb\") as f:\n#         f.write(requests.get(url).content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## colored","metadata":{}},{"cell_type":"code","source":"pathname = './colored'\n# if path doesn't exist, make that path dir\nif not os.path.isdir(pathname):\n    os.makedirs(pathname)\n    \nfor i in tqdm(range(len(cl)), \"downloading images\"):\n    url = cl[i].replace('\\r', '')\n        \n    filename = os.path.join(pathname, str(i) + str('.png'))\n    \n    with open(filename, \"wb\") as f:\n        f.write(requests.get(url).content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## grayscale","metadata":{}},{"cell_type":"code","source":"pathname = './grayscale'\n# if path doesn't exist, make that path dir\nif not os.path.isdir(pathname):\n    os.makedirs(pathname)\n    \nfor i in tqdm(range(len(bw)), \"downloading images\"):\n    url = bw[i].replace('\\r', '')\n    \n    filename = os.path.join(pathname, str(i) + str('.png'))\n    \n    with open(filename, \"wb\") as f:\n        f.write(requests.get(url).content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}